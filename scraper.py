import os
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import re
from urllib.parse import urlparse # æ–°å¢å¯¼å…¥ï¼Œç”¨äºè§£æURLè·å–æ–‡ä»¶æ‰©å±•å

# --- é…ç½® ---
URL = "https://t.me/s/zaihuapd"
OUTPUT_DIR = "src/content/posts"
IMAGE_OUTPUT_DIR = "src/assets/images" # æ–°å¢ï¼šå›¾ç‰‡ä¿å­˜ç›®å½•
# --- ç»“æŸé…ç½® ---

def download_image(url, post_id):
    """ä¸‹è½½å›¾ç‰‡å¹¶ä¿å­˜åˆ°æœ¬åœ°"""
    if not url:
        return None

    try:
        # ä»URLä¸­æå–æ–‡ä»¶æ‰©å±•åï¼Œå¤„ç†æ‰å¯èƒ½çš„æŸ¥è¯¢å‚æ•°
        path = urlparse(url).path
        ext = os.path.splitext(path)[1]
        if not ext: # å¦‚æœæ²¡æœ‰æ‰©å±•åï¼Œé»˜è®¤ä½¿ç”¨ .jpg
            ext = '.jpg'
            
        filename = f"{post_id}{ext}"
        filepath = os.path.join(IMAGE_OUTPUT_DIR, filename)

        # å‘é€è¯·æ±‚ä¸‹è½½å›¾ç‰‡
        img_response = requests.get(url, stream=True, timeout=20)
        img_response.raise_for_status()

        # ä»¥äºŒè¿›åˆ¶å†™æ¨¡å¼ä¿å­˜å›¾ç‰‡æ–‡ä»¶
        with open(filepath, 'wb') as f:
            for chunk in img_response.iter_content(chunk_size=8192):
                f.write(chunk)
        
        print(f"ğŸ–¼ï¸  æˆåŠŸä¸‹è½½å›¾ç‰‡: {filepath}")
        return filepath # è¿”å›æœ¬åœ°æ–‡ä»¶è·¯å¾„
    except requests.RequestException as e:
        print(f"âŒ ä¸‹è½½å›¾ç‰‡å¤±è´¥ {url}: {e}")
        return None
    except Exception as e:
        print(f"âŒ ä¿å­˜å›¾ç‰‡æ—¶å‡ºé”™: {e}")
        return None

def create_markdown_file(post_id, title, published_date, local_image_path, body_content, source_text):
    """æ ¹æ®æå–çš„ä¿¡æ¯ç”Ÿæˆ Markdown æ–‡ä»¶å†…å®¹"""
    
    clean_title = title.replace("'", "â€™").replace('"', 'â€').strip()
    description = clean_title

    # ã€ä¿®æ”¹ç‚¹ã€‘æ ¹æ®æœ¬åœ°å›¾ç‰‡è·¯å¾„ç”Ÿæˆç›¸å¯¹è·¯å¾„
    image_frontmatter = "image: ''"
    if local_image_path:
        image_filename = os.path.basename(local_image_path)
        # ç›¸å¯¹è·¯å¾„ï¼šä» posts/ -> ../ -> assets/images/
        relative_image_path = f"../assets/images/{image_filename}"
        image_frontmatter = f"image: '{relative_image_path}'"

    # æ­£æ–‡éƒ¨åˆ†ä¸å†åŒ…å«å›¾ç‰‡æ ‡ç­¾
    body_markdown = f"## {title}\n\n{body_content.strip()}\n\n"
    if source_text:
        body_markdown += f"*{source_text.strip()}*"

    content = f"""---
title: '{clean_title}'
published: {published_date}
description: '{description}'
{image_frontmatter}
tags: [ç§‘æŠ€é¢‘é“]
category: 'ç§‘æŠ€é¢‘é“'
draft: false
lang: ''
---

{body_markdown}
"""
    
    filename = f"kjpd{post_id}.md"
    filepath = os.path.join(OUTPUT_DIR, filename)
    
    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)
        print(f"âœ… æˆåŠŸåˆ›å»ºæ–‡ä»¶: {filepath}")
    except Exception as e:
        print(f"âŒ åˆ›å»ºæ–‡ä»¶æ—¶å‡ºé”™ {filepath}: {e}")

def scrape_news():
    """ä¸»çˆ¬è™«å‡½æ•°"""
    print("ğŸš€ å¼€å§‹çˆ¬å–æ–°é—»...")
    
    # ã€ä¿®æ”¹ç‚¹ã€‘ç¡®ä¿ä¸¤ä¸ªè¾“å‡ºç›®å½•éƒ½å­˜åœ¨
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    os.makedirs(IMAGE_OUTPUT_DIR, exist_ok=True)
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }

    try:
        response = requests.get(URL, headers=headers, timeout=20)
        response.raise_for_status()
    except requests.RequestException as e:
        print(f"âŒ è®¿é—® {URL} å¤±è´¥: {e}")
        return

    soup = BeautifulSoup(response.text, 'html.parser')
    messages = soup.find_all('div', class_='tgme_widget_message_wrap')
    
    if not messages:
        print("ğŸŸ¡ æœªæ‰¾åˆ°ä»»ä½•æ–°é—»æ¶ˆæ¯ã€‚")
        return

    print(f"ğŸ” æ‰¾åˆ° {len(messages)} æ¡æ¶ˆæ¯ï¼Œå¼€å§‹å¤„ç†...")
    new_posts_count = 0
    for message_div in reversed(messages):
        widget_message = message_div.find('div', class_='js-widget_message')
        if not widget_message or 'data-post' not in widget_message.attrs:
            continue

        post_id = widget_message['data-post'].split('/')[-1]
        filename = f"kjpd{post_id}.md"
        filepath = os.path.join(OUTPUT_DIR, filename)

        if os.path.exists(filepath):
            continue
        
        new_posts_count += 1
        print(f"ğŸ†• å‘ç°æ–°æ–‡ç« ï¼ŒID: {post_id}")

        message_text_div = widget_message.find('div', class_='js-message_text')
        if not message_text_div:
            continue

        title_tag = message_text_div.find('b')
        title = title_tag.get_text(strip=True) if title_tag else f"æ— æ ‡é¢˜æ–°é—» - {post_id}"

        # æå–å›¾ç‰‡ URL
        image_url = ''
        photo_wrap = widget_message.find('a', class_='tgme_widget_message_photo_wrap')
        if photo_wrap and 'style' in photo_wrap.attrs:
            match = re.search(r"url\('(.+?)'\)", photo_wrap['style'])
            if match:
                image_url = match.group(1)
        
        # ã€ä¿®æ”¹ç‚¹ã€‘ä¸‹è½½å›¾ç‰‡å¹¶è·å–æœ¬åœ°è·¯å¾„
        local_image_path = download_image(image_url, post_id)

        # æå–æ­£æ–‡å’Œæ¥æº... (é€»è¾‘ä¸å˜)
        content_soup = BeautifulSoup(str(message_text_div), 'html.parser')
        if content_soup.b:
            content_soup.b.decompose()
        source_text = ''
        for link in reversed(content_soup.find_all('a')):
            if 't.me/' not in link.get('href', ''):
                source_text = link.get_text(strip=True)
                break
        for tag in content_soup.find_all(['a', 'i']):
            tag.decompose()
        body_content = content_soup.get_text(separator='\n', strip=True)
        body_content = re.sub(r'ç¾¤å‹æŠ•ç¨¿è¡¥å……\s*', '', body_content)
        
        # ç”Ÿæˆ Markdown æ–‡ä»¶
        published_date = datetime.now().strftime('%Y-%m-%d')
        # ã€ä¿®æ”¹ç‚¹ã€‘ä¼ å…¥æœ¬åœ°å›¾ç‰‡è·¯å¾„
        create_markdown_file(post_id, title, published_date, local_image_path, body_content, source_text)

    if new_posts_count == 0:
        print("âœ… æ‰€æœ‰æ–°é—»éƒ½å·²æ˜¯æœ€æ–°ï¼Œæ— éœ€æ›´æ–°ã€‚")
    else:
        print(f"ğŸ‰ å¤„ç†å®Œæˆï¼Œæ–°å¢ {new_posts_count} ç¯‡æ–‡ç« ã€‚")

if __name__ == "__main__":
    scrape_news()