---
title: 小米发布 MiMo-V2-Flash 大模型，采用混合专家架构实现高效推理
published: 2025-12-16
description: '小米发布 MiMo-V2-Flash 大模型，采用混合专家架构实现高效推理'
image: ../../assets/images/38236.jpg
tags: [科技频道]
category: '科技频道'
draft: false
lang: ''
---

## 小米发布 MiMo-V2-Flash 大模型，采用混合专家架构实现高效推理

小米发布了 MiMo-V2-Flash 大模型，这是一个采用混合专家（MoE）架构的语言模型，总参数量达 309B，激活参数为 15B。该模型专为高速推理和智能体工作流设计，通过混合注意力架构和多令牌预测技术，在显著降低推理成本的同时实现了业界领先的性能。
MiMo-V2-Flash 的核心特性包括混合注意力架构，以 5:1 的比例交替使用滑动窗口注意力和全局注意力，KV 缓存存储减少近 6 倍；多令牌预测模块使推理输出速度提升 3 倍；支持最长 256K 的上下文窗口。该模型在多项基准测试中表现优异，在 SWE-Bench 等复杂推理任务上超越了参数量更大的竞品模型。小米已在 Hugging Face 平台开源该模型的基础版本。

*Hugging Face*
