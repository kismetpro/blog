import os
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import re

# --- é…ç½® ---
# ç›®æ ‡ Telegram é¢‘é“ç½‘é¡µ
URL = "https://t.me/s/zaihuapd"
# Markdown æ–‡ä»¶è¾“å‡ºç›®å½•
OUTPUT_DIR = "src/content/posts"
# --- ç»“æŸé…ç½® ---

def create_markdown_file(post_id, title, published_date, image_url, body_content, source_text):
    """æ ¹æ®æå–çš„ä¿¡æ¯ç”Ÿæˆ Markdown æ–‡ä»¶å†…å®¹"""
    
    # æ¸…ç†æ ‡é¢˜å’Œæè¿°ä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼Œé¿å…åœ¨ frontmatter ä¸­å¼•èµ·é—®é¢˜
    clean_title = title.replace("'", "â€™").replace('"', 'â€').strip()
    # æè¿°å¯ä»¥ç®€å•åœ°ä½¿ç”¨æ ‡é¢˜
    description = clean_title

    # frontmatter ä¸­çš„ image å­—æ®µï¼Œå¦‚æœå›¾ç‰‡å­˜åœ¨åˆ™å¡«å…¥ URLï¼Œå¦åˆ™ä¸ºç©º
    image_frontmatter = f"image: '{image_url}'" if image_url else "image: ''"

    # æ ¼å¼åŒ–æ­£æ–‡
    # ã€ä¿®æ”¹ç‚¹ã€‘ä¸å†å‘æ­£æ–‡ä¸­æ·»åŠ  ![alt](url) å›¾ç‰‡æ ‡ç­¾
    body_markdown = ""
    body_markdown += f"## {title}\n\n"
    body_markdown += f"{body_content.strip()}\n\n"
    
    if source_text:
        # å¦‚æœæ¥æºæ–‡æœ¬å­˜åœ¨ï¼Œä½¿ç”¨æ–œä½“æ ¼å¼åŒ–
        body_markdown += f"*{source_text.strip()}*"

    # ç»„è£…å®Œæ•´çš„ Markdown æ–‡ä»¶å†…å®¹
    content = f"""---
title: '{clean_title}'
published: {published_date}
description: '{description}'
{image_frontmatter}
tags: [ç§‘æŠ€é¢‘é“]
category: 'ç§‘æŠ€é¢‘é“'
draft: false
lang: ''
---

{body_markdown}
"""
    
    # å®šä¹‰æ–‡ä»¶åå¹¶ä¿å­˜
    filename = f"kjpd{post_id}.md"
    filepath = os.path.join(OUTPUT_DIR, filename)
    
    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)
        print(f"âœ… æˆåŠŸåˆ›å»ºæ–‡ä»¶: {filepath}")
    except Exception as e:
        print(f"âŒ åˆ›å»ºæ–‡ä»¶æ—¶å‡ºé”™ {filepath}: {e}")


def scrape_news():
    """ä¸»çˆ¬è™«å‡½æ•°"""
    print("ğŸš€ å¼€å§‹çˆ¬å–æ–°é—»...")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }

    try:
        response = requests.get(URL, headers=headers, timeout=20)
        response.raise_for_status()  # å¦‚æœè¯·æ±‚å¤±è´¥åˆ™æŠ›å‡ºå¼‚å¸¸
    except requests.RequestException as e:
        print(f"âŒ è®¿é—® {URL} å¤±è´¥: {e}")
        return

    soup = BeautifulSoup(response.text, 'html.parser')
    
    # æŸ¥æ‰¾æ‰€æœ‰æ–°é—»æ¶ˆæ¯çš„å®¹å™¨
    messages = soup.find_all('div', class_='tgme_widget_message_wrap')
    
    if not messages:
        print("ğŸŸ¡ æœªæ‰¾åˆ°ä»»ä½•æ–°é—»æ¶ˆæ¯ã€‚")
        return

    print(f"ğŸ” æ‰¾åˆ° {len(messages)} æ¡æ¶ˆæ¯ï¼Œå¼€å§‹å¤„ç†...")

    new_posts_count = 0
    for message_div in reversed(messages):  # ä»æ—§åˆ°æ–°å¤„ç†ï¼Œç¬¦åˆæ—¶é—´é¡ºåº
        widget_message = message_div.find('div', class_='js-widget_message')
        if not widget_message or 'data-post' not in widget_message.attrs:
            continue

        # 1. åˆ¤æ–­æ–°é—»æ˜¯å¦å·²è¢«çˆ¬å–
        post_id = widget_message['data-post'].split('/')[-1]
        filename = f"kjpd{post_id}.md"
        filepath = os.path.join(OUTPUT_DIR, filename)

        if os.path.exists(filepath):
            # print(f"â­ï¸ è·³è¿‡å·²å­˜åœ¨çš„æ–°é—»: {post_id}")
            continue
        
        new_posts_count += 1
        print(f"ğŸ†• å‘ç°æ–°æ–‡ç« ï¼ŒID: {post_id}")

        # 2. æå–æ–°é—»å†…å®¹
        message_text_div = widget_message.find('div', class_='js-message_text')
        if not message_text_div:
            continue

        # æå–æ ‡é¢˜ (é€šå¸¸æ˜¯ç¬¬ä¸€ä¸ª <b> æ ‡ç­¾)
        title_tag = message_text_div.find('b')
        title = title_tag.get_text(strip=True) if title_tag else f"æ— æ ‡é¢˜æ–°é—» - {post_id}"

        # æå–å›¾ç‰‡ URL
        image_url = ''
        photo_wrap = widget_message.find('a', class_='tgme_widget_message_photo_wrap')
        if photo_wrap and 'style' in photo_wrap.attrs:
            style = photo_wrap['style']
            match = re.search(r"url\('(.+?)'\)", style)
            if match:
                image_url = match.group(1)

        # æå–æ­£æ–‡å’Œæ¥æº
        # å¤åˆ¶ä¸€ä»½ç”¨äºæ“ä½œï¼Œé¿å…ç ´ååŸå§‹ç»“æ„
        content_soup = BeautifulSoup(str(message_text_div), 'html.parser')
        
        # ç§»é™¤æ ‡é¢˜ <b>
        if content_soup.b:
            content_soup.b.decompose()
        
        # æŸ¥æ‰¾æ¥æºé“¾æ¥ (æœ€åä¸€ä¸ªä¸æ˜¯ Telegram å†…éƒ¨é“¾æ¥çš„ a æ ‡ç­¾)
        source_text = ''
        all_links = content_soup.find_all('a')
        source_link_tag = None
        for link in reversed(all_links):
            href = link.get('href', '')
            if 't.me/' not in href:
                source_text = link.get_text(strip=True)
                source_link_tag = link
                break
        
        # ç§»é™¤æ‰€æœ‰é“¾æ¥å’Œé¢‘é“å›ºå®šé¡µè„šï¼Œä»¥è·å¾—çº¯å‡€çš„æ­£æ–‡
        for tag in content_soup.find_all(['a', 'i']):
            tag.decompose()
            
        # ç§»é™¤ç‰¹å®šæ–‡æœ¬æ¨¡å¼çš„é¡µè„šï¼ˆä¾‹å¦‚ç¾¤å‹æŠ•ç¨¿è¡¥å……ï¼‰
        body_content = content_soup.get_text(separator='\n', strip=True)
        body_content = re.sub(r'ç¾¤å‹æŠ•ç¨¿è¡¥å……\s*', '', body_content) # ç§»é™¤ç¾¤å‹æŠ•ç¨¿æç¤º
        
        # 3. ç”Ÿæˆ Markdown æ–‡ä»¶
        published_date = datetime.now().strftime('%Y-%m-%d')
        create_markdown_file(post_id, title, published_date, image_url, body_content, source_text)

    if new_posts_count == 0:
        print("âœ… æ‰€æœ‰æ–°é—»éƒ½å·²æ˜¯æœ€æ–°ï¼Œæ— éœ€æ›´æ–°ã€‚")
    else:
        print(f"ğŸ‰ å¤„ç†å®Œæˆï¼Œæ–°å¢ {new_posts_count} ç¯‡æ–‡ç« ã€‚")


if __name__ == "__main__":
    scrape_news()